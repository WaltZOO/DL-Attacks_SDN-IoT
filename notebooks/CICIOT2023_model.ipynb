{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e331fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5125e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV: 0.02% complete"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../datasets/CICIOT2023/merged_CICIOT2023.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39mchunksize) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[0;32m----> 7\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, chunk], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m         progress \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m13754096325\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mReading CSV: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[1;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    686\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "chunksize = 10 ** 3\n",
    "\n",
    "# read the dataset by splitting it by chunks because this dataset is too large (13GB), it leads to kernel crashes\n",
    "with pd.read_csv('../../datasets/CICIOT2023/merged_CICIOT2023.csv', chunksize=chunksize) as reader:\n",
    "    for chunk in reader:\n",
    "        df = pd.concat([df, chunk], ignore_index=True)\n",
    "        progress = (len(df) / 13754096325) * 100\n",
    "        print(f\"\\rReading CSV: {progress:.2f}% complete\", end=\"\")\n",
    "\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004478d",
   "metadata": {},
   "source": [
    "### Missing datas\n",
    "useless here because no missing datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f04aaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing datas by columns :\n",
      "  Destination Port              0\n",
      " Flow Duration                 0\n",
      " Total Fwd Packets             0\n",
      " Total Backward Packets        0\n",
      "Total Length of Fwd Packets    0\n",
      "                              ..\n",
      "Idle Mean                      0\n",
      " Idle Std                      0\n",
      " Idle Max                      0\n",
      " Idle Min                      0\n",
      " Label                         0\n",
      "Length: 79, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer # to handle missing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Missing datas by columns :\\n\", df.isnull().sum())\n",
    "\n",
    "#here there is no missing data so we don't have to manage this\n",
    "\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b5c8b",
   "metadata": {},
   "source": [
    "### Separing datas and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ceb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip() # to clean spaces on the columns names\n",
    "X = df.drop(columns=['Label']) # without labels\n",
    "Y = df['Label'] # just labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365d58c",
   "metadata": {},
   "source": [
    "### Encoding categorical data\n",
    "Difference between Label and OneHot : \\\n",
    "                                      - Label gives a number in int for each line (simple but the model could misunderstand the difference between numbers (priorities issues))\\\n",
    "                                      - OneHot encodes in binary columns (no hierarchy between column but a lot of columns if they are a lot of categories to label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0\n",
      "1          0\n",
      "2          0\n",
      "3          0\n",
      "4          0\n",
      "          ..\n",
      "2830738    0\n",
      "2830739    0\n",
      "2830740    0\n",
      "2830741    0\n",
      "2830742    0\n",
      "Name: Label, Length: 2830743, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# no need of OneHotEncoder because all columns except Label are numerical columns\n",
    "\n",
    "Y = Y.apply(lambda x: 0 if x == 'BENIGN' else 1) # because we have several types of attacks and we wants bianaries Y\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52c71f",
   "metadata": {},
   "source": [
    "### Splitting into training set and test set\n",
    "We are now splitting the dataset\n",
    "The train set that has the full data to train and the test set which has only 3 columns for testing on smaller samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d956c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Destination Port  Flow Duration  Total Fwd Packets  \\\n",
      "2092860              53.0        64428.0                2.0   \n",
      "2140044             443.0       757414.0               12.0   \n",
      "1590488              53.0       184165.0                4.0   \n",
      "128477            54342.0      1863306.0                6.0   \n",
      "1336329              53.0          235.0                2.0   \n",
      "...                   ...            ...                ...   \n",
      "2249467              80.0     85562173.0                8.0   \n",
      "963395              443.0    118654701.0               45.0   \n",
      "2215104              80.0     87497358.0                8.0   \n",
      "1484405             443.0      1543512.0                8.0   \n",
      "305711               80.0    115280042.0               16.0   \n",
      "\n",
      "         Total Backward Packets  Total Length of Fwd Packets  \\\n",
      "2092860                     2.0                         78.0   \n",
      "2140044                     8.0                        545.0   \n",
      "1590488                     2.0                        132.0   \n",
      "128477                      2.0                      11607.0   \n",
      "1336329                     2.0                         62.0   \n",
      "...                         ...                          ...   \n",
      "2249467                     5.0                        376.0   \n",
      "963395                     78.0                        734.0   \n",
      "2215104                     5.0                        350.0   \n",
      "1484405                    10.0                        703.0   \n",
      "305711                     14.0                        419.0   \n",
      "\n",
      "         Total Length of Bwd Packets  Fwd Packet Length Max  \\\n",
      "2092860                        148.0                   39.0   \n",
      "2140044                       3854.0                  199.0   \n",
      "1590488                        246.0                   33.0   \n",
      "128477                          26.0                 7215.0   \n",
      "1336329                         94.0                   31.0   \n",
      "...                              ...                    ...   \n",
      "2249467                      11595.0                  370.0   \n",
      "963395                      136087.0                  353.0   \n",
      "2215104                      11595.0                  350.0   \n",
      "1484405                       3950.0                  267.0   \n",
      "305711                        1767.0                  419.0   \n",
      "\n",
      "         Fwd Packet Length Min  Fwd Packet Length Mean  Fwd Packet Length Std  \\\n",
      "2092860                   39.0               39.000000               0.000000   \n",
      "2140044                    0.0               45.416667              72.054851   \n",
      "1590488                   33.0               33.000000               0.000000   \n",
      "128477                     0.0             1934.500000            3123.682298   \n",
      "1336329                   31.0               31.000000               0.000000   \n",
      "...                        ...                     ...                    ...   \n",
      "2249467                    0.0               47.000000             130.528596   \n",
      "963395                     0.0               16.311111              63.099064   \n",
      "2215104                    0.0               43.750000             123.743687   \n",
      "1484405                    0.0               87.875000             105.902162   \n",
      "305711                     0.0               26.187500             104.750000   \n",
      "\n",
      "         ...  act_data_pkt_fwd  min_seg_size_forward   Active Mean  \\\n",
      "2092860  ...               1.0                  20.0      0.000000   \n",
      "2140044  ...              11.0                  20.0      0.000000   \n",
      "1590488  ...               3.0                  20.0      0.000000   \n",
      "128477   ...               4.0                  20.0      0.000000   \n",
      "1336329  ...               1.0                  20.0      0.000000   \n",
      "...      ...               ...                   ...           ...   \n",
      "2249467  ...               2.0                  20.0   2989.000000   \n",
      "963395   ...               4.0                  32.0  35395.416667   \n",
      "2215104  ...               1.0                  32.0      8.000000   \n",
      "1484405  ...               4.0                  32.0      0.000000   \n",
      "305711   ...               1.0                  32.0  42530.363640   \n",
      "\n",
      "           Active Std  Active Max  Active Min     Idle Mean       Idle Std  \\\n",
      "2092860      0.000000         0.0         0.0  0.000000e+00       0.000000   \n",
      "2140044      0.000000         0.0         0.0  0.000000e+00       0.000000   \n",
      "1590488      0.000000         0.0         0.0  0.000000e+00       0.000000   \n",
      "128477       0.000000         0.0         0.0  0.000000e+00       0.000000   \n",
      "1336329      0.000000         0.0         0.0  0.000000e+00       0.000000   \n",
      "...               ...         ...         ...           ...            ...   \n",
      "2249467      0.000000      2989.0      2989.0  8.440000e+07       0.000000   \n",
      "963395   40019.555053    162470.0     23573.0  9.852496e+06  577048.345035   \n",
      "2215104      0.000000         8.0         8.0  8.450000e+07       0.000000   \n",
      "1484405      0.000000         0.0         0.0  0.000000e+00       0.000000   \n",
      "305711   12309.390600     79643.0     38700.0  1.000000e+07    3587.066994   \n",
      "\n",
      "           Idle Max    Idle Min  \n",
      "2092860         0.0         0.0  \n",
      "2140044         0.0         0.0  \n",
      "1590488         0.0         0.0  \n",
      "128477          0.0         0.0  \n",
      "1336329         0.0         0.0  \n",
      "...             ...         ...  \n",
      "2249467  84400000.0  84400000.0  \n",
      "963395   10024451.0   8020740.0  \n",
      "2215104  84500000.0  84500000.0  \n",
      "1484405         0.0         0.0  \n",
      "305711   10000000.0   9998346.0  \n",
      "\n",
      "[1981520 rows x 78 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9e2f1",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "To have the same scale between each columns (for instance, `Flow Duration` is way bigger than `Tot Fwd Pkts` and the model could misinterpret it and gives more importance to the Income column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
