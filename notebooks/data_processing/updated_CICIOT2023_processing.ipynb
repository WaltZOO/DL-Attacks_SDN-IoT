{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ebafb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 12:06:49.656835: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee96edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167994/4204261455.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../../datasets/CICIOT2023/full_version/lighter_CICIOT2023.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/CICIOT2023/full_version/lighter_CICIOT2023.csv')\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1f6aa",
   "metadata": {},
   "source": [
    "### Dropping useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa1113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              flow_duration Header_Length  ...  Weight             label\n",
      "0                       0.0          54.0  ...  141.55    DDoS-TCP_Flood\n",
      "1                       0.0           0.0  ...  141.55   DDoS-ICMP_Flood\n",
      "2                   0.07496       31247.0  ...  141.55    DDoS-UDP_Flood\n",
      "3                       0.0           0.0  ...  141.55   DDoS-ICMP_Flood\n",
      "4        3.5216370820999146      171704.8  ...    38.5     BenignTraffic\n",
      "...                     ...           ...  ...     ...               ...\n",
      "2103094                 0.0           0.0  ...  141.55   DDoS-ICMP_Flood\n",
      "2103095                 0.0          54.0  ...  141.55    DDoS-SYN_Flood\n",
      "2103096            0.242968      34894.19  ...  141.55     DoS-UDP_Flood\n",
      "2103097            0.185182         65.52  ...  141.55    DDoS-SYN_Flood\n",
      "2103098                 0.0          54.0  ...  141.55  DDoS-RSTFINFlood\n",
      "\n",
      "[2103099 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74002ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useless columns : []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#Drop useless columns\n",
    "\n",
    "useless_column = []\n",
    "# columns where we have always the same value\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() == 1:\n",
    "        useless_column.append(col)\n",
    "\n",
    "print(f\"Useless columns : {useless_column}\")\n",
    "\n",
    "cols_to_drop = [\n",
    "    \"Protocol Type\",\n",
    "    \"Magnitue\", \"Radius\", \"Covariance\", \"Variance\", \"Weight\",\n",
    "]\n",
    "df.drop(columns=cols_to_drop)\n",
    "df.drop(columns=useless_column, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102cede9",
   "metadata": {},
   "source": [
    "### Balancing the datasets (subsamples attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5dc4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875895\n",
      "Before balancing, there is 113602 normal traffic and 1989497 attacks.\n",
      "\n",
      "After balancing, there is 113602 normal traffic and 113602 attacks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"label\"] = df[\"label\"].str.strip()\n",
    "\n",
    "N_BenignTraffic = df[df[\"label\"] == \"BenignTraffic\"].shape[0]\n",
    "N_Attacks = df[df[\"label\"] != \"BenignTraffic\"].shape[0]\n",
    "N = N_Attacks-N_BenignTraffic\n",
    "print(N)\n",
    "\n",
    "attacks = df[df[\"label\"] != \"BenignTraffic\"]\n",
    "to_remove = attacks.sample(n=N, random_state=42)\n",
    "\n",
    "print(f\"Before balancing, there are {df[df['label'] == 'BenignTraffic'].shape[0]} normal traffic and {attacks.shape[0]} attacks.\\n\")\n",
    "\n",
    "df = df.drop(index=to_remove.index)\n",
    "\n",
    "print(f\"After balancing, there are {df[df['label'] == 'BenignTraffic'].shape[0]} normal traffic and {df[df['label'] != 'BenignTraffic'].shape[0]} attacks.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04c303",
   "metadata": {},
   "source": [
    "### Separing datas and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f441d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    113602\n",
      "1    113602\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip() # to clean spaces on the columns names\n",
    "X = df.drop(columns=['label']) # without labels\n",
    "Y = df['label'] # just labels\n",
    "\n",
    "Y = Y.apply(lambda x: 0 if x == 'BenignTraffic' else 1)\n",
    "print(Y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a15e75b",
   "metadata": {},
   "source": [
    "### Encoding categorical data\n",
    "Difference between Label and OneHot : \\\n",
    "                                      - Label gives a number in int for each line (simple but the model could misunderstand the difference between numbers (priorities issues))\\\n",
    "                                      - OneHot encodes in binary columns (no hierarchy between column but a lot of columns if they are a lot of categories to label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4757c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# no need of OneHotEncoder because all columns are numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ef427",
   "metadata": {},
   "source": [
    "### Splitting into training set and test set\n",
    "Split dataset into training and testing sets (70/30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c36621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              flow_duration Header_Length  ... Variance  Weight\n",
      "561384                  0.0          54.0  ...      0.0  141.55\n",
      "1590905    55.5097695350647       55325.9  ...      0.9    38.5\n",
      "352239     30.8857843875885     1224697.6  ...      0.8    38.5\n",
      "1649343            0.007652        4654.4  ...     0.12  141.55\n",
      "66704              3.473864       12588.2  ...      0.8    38.5\n",
      "...                     ...           ...  ...      ...     ...\n",
      "731132             8.626672      222138.6  ...      1.0   244.6\n",
      "707625            42.032916      119843.4  ...      0.4    38.5\n",
      "978586            15.936039      762842.5  ...      1.0   244.6\n",
      "1362398  0.0128752350807189        9813.0  ...      0.0  141.55\n",
      "1552405           20.166727     1125992.4  ...      1.0   244.6\n",
      "\n",
      "[68162 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0, stratify=Y)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c17c74c",
   "metadata": {},
   "source": [
    "### Missing datas\n",
    "useless here because no missing datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69a7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer # to handle missing data\n",
    "\n",
    "#here there is no missing data so we don't have to manage this\n",
    "\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "#because numeric values are not really numeric in the csv, so we convert them\n",
    "\n",
    "\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4bf9a8",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "To have the same scale between each columns (for instance, `Flow Duration` is way bigger than `Tot Fwd Pkts` and the model could misinterpret it and gives more importance to the Income column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e13473f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e724bc",
   "metadata": {},
   "source": [
    "### Prepare data for Deep Learning (convert datas into float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7a186c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../preprocessed_data/CICIOT_update/Y_test.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "Y_train = np.array(Y_train).astype('float32')\n",
    "Y_test = np.array(Y_test).astype('float32')\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "joblib.dump(X_train, '../../preprocessed_data/CICIOT_update/X_train.joblib')\n",
    "joblib.dump(X_test, '../../preprocessed_data/CICIOT_update/X_test.joblib')\n",
    "joblib.dump(Y_train, '../../preprocessed_data/CICIOT_update/Y_train.joblib')\n",
    "joblib.dump(Y_test, '../../preprocessed_data/CICIOT_update/Y_test.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDNIOT-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
