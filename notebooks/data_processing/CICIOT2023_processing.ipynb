{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d020b2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:16:09.134556: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f869e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = pd.read_csv('../../datasets/CICIOT2023/tharindu_cleaned_version/nattack.csv')\n",
    "benign = pd.read_csv('../../datasets/CICIOT2023/tharindu_cleaned_version/nbengin.csv')\n",
    "\n",
    "df = pd.concat([attacks, benign], ignore_index=True)\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ea641",
   "metadata": {},
   "source": [
    "### Dropping useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5013dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useless columns : ['Protocol Type']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Drop useless columns\n",
    "\n",
    "useless_column = []\n",
    "# columns where we have always the same value\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() == 1:\n",
    "        useless_column.append(col)\n",
    "\n",
    "print(f\"Useless columns : {useless_column}\")\n",
    "\n",
    "df.drop(columns=useless_column, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e540f4",
   "metadata": {},
   "source": [
    "### Separing datas and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94c24a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "1    536894\n",
      "0    517191\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip() # to clean spaces on the columns names\n",
    "X = df.drop(columns=['Label']) # without labels\n",
    "Y = df['Label'] # just labels\n",
    "\n",
    "print(Y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71b3ca",
   "metadata": {},
   "source": [
    "### Encoding categorical data\n",
    "Difference between Label and OneHot : \\\n",
    "                                      - Label gives a number in int for each line (simple but the model could misunderstand the difference between numbers (priorities issues))\\\n",
    "                                      - OneHot encodes in binary columns (no hierarchy between column but a lot of columns if they are a lot of categories to label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bfffe7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# no need of OneHotEncoder because all columns are numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261170ac",
   "metadata": {},
   "source": [
    "### Splitting into training set and test set\n",
    "Split dataset into training and testing sets (70/30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "38fa7e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ca549",
   "metadata": {},
   "source": [
    "### Missing datas\n",
    "useless here because no missing datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "96b4985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer # to handle missing data\n",
    "\n",
    "#here there is no missing data so we don't have to manage this\n",
    "\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9272377e",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "To have the same scale between each columns (for instance, `Flow Duration` is way bigger than `Tot Fwd Pkts` and the model could misinterpret it and gives more importance to the Income column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a56dc730",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7db1ee",
   "metadata": {},
   "source": [
    "### Prepare data for Deep Learning (convert datas into float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d8a51dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../preprocessed_data/CICIOT/Y_test.joblib']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "Y_train = np.array(Y_train).astype('float32')\n",
    "Y_test = np.array(Y_test).astype('float32')\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "joblib.dump(X_train, '../../preprocessed_data/CICIOT/X_train.joblib')\n",
    "joblib.dump(X_test, '../../preprocessed_data/CICIOT/X_test.joblib')\n",
    "joblib.dump(Y_train, '../../preprocessed_data/CICIOT/Y_train.joblib')\n",
    "joblib.dump(Y_test, '../../preprocessed_data/CICIOT/Y_test.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDNIOT-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
