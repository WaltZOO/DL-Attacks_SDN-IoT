{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac4ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc7cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = pd.read_csv('../../datasets/InSDN_DatasetCSV/Normal_data.csv')\n",
    "ovs = pd.read_csv('../../datasets/InSDN_DatasetCSV/OVS.csv')\n",
    "metasploitable = pd.read_csv('../../datasets/InSDN_DatasetCSV/metasploitable-2.csv')\n",
    "\n",
    "df = pd.concat([normal, ovs, metasploitable], ignore_index=True)\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03653c56",
   "metadata": {},
   "source": [
    "### Dropping useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec0b1a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useless columns : ['Fwd PSH Flags', 'Fwd URG Flags', 'CWE Flag Count', 'ECE Flag Cnt', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Init Fwd Win Byts', 'Fwd Seg Size Min']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#Drop useless columns\n",
    "\n",
    "useless_column = []\n",
    "# columns where we have always the same value\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() == 1:\n",
    "        useless_column.append(col)\n",
    "\n",
    "print(f\"Useless columns : {useless_column}\")\n",
    "\n",
    "df.drop(columns=useless_column, inplace=True)\n",
    "\n",
    "df = df.drop(columns=['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac30011",
   "metadata": {},
   "source": [
    "### Balancing the datasets (subsamples attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87a2b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207041\n",
      "Before balancing, there is 68424 normal traffic and 275465 attacks.\n",
      "\n",
      "After balancing, there is 68424 normal traffic and 68424 attacks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"Label\"] = df[\"Label\"].str.strip()\n",
    "\n",
    "N_Normal = df[df[\"Label\"] == \"Normal\"].shape[0]\n",
    "N_Attacks = df[df[\"Label\"] != \"Normal\"].shape[0]\n",
    "N = N_Attacks-N_Normal\n",
    "print(N)\n",
    "\n",
    "attacks = df[df[\"Label\"] != \"Normal\"]\n",
    "to_remove = attacks.sample(n=N, random_state=42)\n",
    "\n",
    "print(f\"Before balancing, there is {df[df['Label'] == 'Normal'].shape[0]} normal traffic and {attacks.shape[0]} attacks.\\n\")\n",
    "\n",
    "df = df.drop(index=to_remove.index)\n",
    "\n",
    "print(f\"After balancing, there is {df[df['Label'] == 'Normal'].shape[0]} normal traffic and {df[df['Label'] != 'Normal'].shape[0]} attacks.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7826dc",
   "metadata": {},
   "source": [
    "### Separing datas and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7249211",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Label']) # without labels\n",
    "Y = df['Label'] # just labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45f507",
   "metadata": {},
   "source": [
    "### Encoding categorical data\n",
    "Difference between Label and OneHot : \\\n",
    "                                      - Label gives a number in int for each line (simple but the model could misunderstand the difference between numbers (priorities issues))\\\n",
    "                                      - OneHot encodes in binary columns (no hierarchy between column but a lot of columns if they are a lot of categories to label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b22189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "1    68424\n",
      "0    68424\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_protocol = encoder.fit_transform(X[['Protocol']])\n",
    "protocol_cols = encoder.get_feature_names_out(['Protocol'])\n",
    "encoded_protocol_df = pd.DataFrame(encoded_protocol, columns=protocol_cols, index=X.index) # convert in dataframe\n",
    "X = pd.concat([X.drop(columns=['Protocol']), encoded_protocol_df], axis=1) #concatenate with the encoded version of protocol\n",
    "\n",
    "# Y_encoder = OneHotEncoder()\n",
    "# Y = Y_encoder.fit_transform(Y)\n",
    "\n",
    "Y = Y.apply(lambda x: 0 if x == 'Normal' else 1) # because we have several types of attacks and we wants bianaries Y\n",
    "\n",
    "print(Y.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb285125",
   "metadata": {},
   "source": [
    "### Splitting into training set and test set\n",
    "Split dataset into training and testing sets (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64ddad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "1    0.500005\n",
      "0    0.499995\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0, stratify=Y)\n",
    "print(Y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1f422",
   "metadata": {},
   "source": [
    "### Missing datas\n",
    "useless here because no missing datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c4ce188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer # to handle missing data\n",
    "\n",
    "num_cols_median = ['Flow Duration', 'Flow Byts/s', 'Fwd Pkt Len Mean', 'Flow IAT Mean']\n",
    "X_train[num_cols_median] = X_train[num_cols_median].fillna(X_train[num_cols_median].median())\n",
    "X_test[num_cols_median] = X_test[num_cols_median].fillna(X_test[num_cols_median].median())\n",
    "\n",
    "cols_fill0 = [\"SYN Flag Cnt\", \"Tot Fwd Pkts\", \"Fwd Act Data Pkts\"]\n",
    "X_train[cols_fill0] = X_train[cols_fill0].fillna(0)\n",
    "X_test[cols_fill0] = X_test[cols_fill0].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598b13a",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "To have the same scale between each columns (for instance, `Flow Duration` is way bigger than `Tot Fwd Pkts` and the model could misinterpret it and gives more importance to the Income column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f59a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961ced0",
   "metadata": {},
   "source": [
    "### Prepare data for Deep Learning (convert datas into float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdd86479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 12:06:39.521730: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../../preprocessed_data/InSDN/Y_test.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "Y_train = np.array(Y_train).astype('float32')\n",
    "Y_test = np.array(Y_test).astype('float32')\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "\n",
    "joblib.dump(X_train, '../../preprocessed_data/InSDN/X_train.joblib')\n",
    "joblib.dump(X_test, '../../preprocessed_data/InSDN/X_test.joblib')\n",
    "joblib.dump(Y_train, '../../preprocessed_data/InSDN/Y_train.joblib')\n",
    "joblib.dump(Y_test, '../../preprocessed_data/InSDN/Y_test.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDNIOT-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
